[
    {
        "title": "Attention Is All You Need",
        "authors": "Vaswani et al.",
        "year": 2017,
        "abstract": "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms..."
    },
    {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "authors": "Devlin et al.",
        "year": 2018,
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers..."
    }
]